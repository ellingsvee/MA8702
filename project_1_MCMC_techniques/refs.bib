@article{roberts_rosenthal_2001,
  title = {Optimal scaling for various {Metropolis-Hastings} algorithms},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  journal = {Statistical Science},
  volume = {16},
  number = {4},
  pages = {351--367},
  year = {2001},
  publisher = {Institute of Mathematical Statistics},
  doi = {10.1214/ss/1015346320},
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James
            Johnson and Chris Leary and Dougal Maclaurin and George Necula and
            Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao
            Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/jax-ml/jax},
  version = {0.3.13},
  year = {2018},
}


@article{dunson_hastings_2020,
  title = {The {Hastings} algorithm at fifty},
  volume = {107},
  issn = {0006-3444},
  url = {https://doi.org/10.1093/biomet/asz066},
  doi = {10.1093/biomet/asz066},
  abstract = {In a 1970 Biometrika paper, W. K. Hastings developed a broad class
              of Markov chain algorithms for sampling from probability
              distributions that are difficult to sample from directly. The
              algorithm draws a candidate value from a proposal distribution and
              accepts the candidate with a probability that can be computed using
              only the unnormalized density of the target distribution, allowing
              one to sample from distributions known only up to a constant of
              proportionality. The stationary distribution of the corresponding
              Markov chain is the target distribution one is attempting to sample
              from. The Hastings algorithm generalizes the Metropolis algorithm
              to allow a much broader class of proposal distributions instead of
              just symmetric cases. An important class of applications for the
              Hastings algorithm corresponds to sampling from Bayesian posterior
              distributions, which have densities given by a prior density
              multiplied by a likelihood function and divided by a normalizing
              constant equal to the marginal likelihood. The marginal likelihood
              is typically intractable, presenting a fundamental barrier to
              implementation in Bayesian statistics. This barrier can be overcome
              by Markov chain Monte Carlo sampling algorithms. Amazingly, even
              after 50 years, the majority of algorithms used in practice today
              involve the Hastings algorithm. This article provides a brief
              celebration of the continuing impact of this ingenious algorithm on
              the 50th anniversary of its publication.},
  number = {1},
  urldate = {2026-01-06},
  journal = {Biometrika},
  author = {Dunson, D B and Johndrow, J E},
  month = mar,
  year = {2020},
  pages = {1--23},
  file = {Full Text PDF:/Users/ellingsv/Zotero/storage/MHE4IGGY/Dunson and
          Johndrow - 2020 - The Hastings algorithm at
          fifty.pdf:application/pdf;Snapshot:/Users/ellingsv/Zotero/storage/VPNXFGKL/asz066.html:text/html
          },
}
