
@article{blei_variational_2017,
  title = {Variational {Inference}: {A} {Review} for {Statisticians}},
  volume = {112},
  issn = {0162-1459, 1537-274X},
  shorttitle = {Variational {Inference}},
  url = {http://arxiv.org/abs/1601.00670},
  doi = {10.1080/01621459.2017.1285773},
  abstract = {One of the core problems of modern statistics is to approximate
              difficult-to-compute probability densities. This problem is
              especially important in Bayesian statistics, which frames all
              inference about unknown quantities as a calculation involving the
              posterior density. In this paper, we review variational inference
              (VI), a method from machine learning that approximates probability
              densities through optimization. VI has been used in many
              applications and tends to be faster than classical methods, such as
              Markov chain Monte Carlo sampling. The idea behind VI is to first
              posit a family of densities and then to find the member of that
              family which is close to the target. Closeness is measured by
              Kullback-Leibler divergence. We review the ideas behind mean-field
              variational inference, discuss the special case of VI applied to
              exponential family models, present a full example with a Bayesian
              mixture of Gaussians, and derive a variant that uses stochastic
              optimization to scale up to massive data. We discuss modern
              research in VI and highlight important open problems. VI is
              powerful, but it is not yet well understood. Our hope in writing
              this paper is to catalyze statistical research on this class of
              algorithms.},
  number = {518},
  urldate = {2026-02-06},
  journal = {Journal of the American Statistical Association},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  month = apr,
  year = {2017},
  note = {arXiv:1601.00670 [stat]},
  keywords = {Statistics - Computation, Computer Science - Machine Learning,
              Statistics - Machine Learning},
  pages = {859--877},
  file = {Preprint PDF:/Users/ellingsv/Zotero/storage/9YT82JX8/Blei et al. -
          2017 - Variational Inference A Review for
          Statisticians.pdf:application/pdf;Snapshot:/Users/ellingsv/Zotero/storage/YP64UPEI/1601.html:text/html
          },
}

@book{bishop2006pattern,
  added-at = {2011-08-03T09:52:39.000+0200},
  author = {Bishop, C.M.},
  biburl = {
            https://www.bibsonomy.org/bibtex/2da4cb0f072a2671bf03b617d5001376c/folke
            },
  interhash = {2c106f24cf8e31f168166791080bfc89},
  intrahash = {da4cb0f072a2671bf03b617d5001376c},
  keywords = {},
  publisher = {Springer New York},
  timestamp = {2011-09-22T11:56:28.000+0200},
  title = {Pattern recognition and machine learning},
  url = {
         http://scholar.google.com/scholar.bib?q=info:jYxggZ6Ag1YJ:scholar.google.com/&output=citation&hl=en&as_sdt=0
         ,5&as_vis=1&ct=citation&cd=0},
  volume = 4,
  year = 2006,
}



@misc{kucukelbir_automatic_2016,
  title = {Automatic {Differentiation} {Variational} {Inference}},
  url = {http://arxiv.org/abs/1603.00788},
  doi = {10.48550/arXiv.1603.00788},
  abstract = {Probabilistic modeling is iterative. A scientist posits a simple
              model, fits it to her data, refines it according to her analysis,
              and repeats. However, fitting complex models to large data is a
              bottleneck in this process. Deriving algorithms for new models can
              be both mathematically and computationally challenging, which makes
              it difficult to efficiently cycle through the steps. To this end,
              we develop automatic differentiation variational inference (ADVI).
              Using our method, the scientist only provides a probabilistic model
              and a dataset, nothing else. ADVI automatically derives an
              efficient variational inference algorithm, freeing the scientist to
              refine and explore many models. ADVI supports a broad class of
              models-no conjugacy assumptions are required. We study ADVI across
              ten different models and apply it to a dataset with millions of
              observations. ADVI is integrated into Stan, a probabilistic
              programming system; it is available for immediate use.},
  urldate = {2026-02-14},
  publisher = {arXiv},
  author = {Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman,
            Andrew and Blei, David M.},
  month = mar,
  year = {2016},
  note = {arXiv:1603.00788 [stat]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science -
              Machine Learning, Statistics - Computation, Statistics - Machine
              Learning},
  file = {Preprint PDF:/Users/ellingsv/Zotero/storage/MGIWX9FQ/Kucukelbir et al.
          - 2016 - Automatic Differentiation Variational
          Inference.pdf:application/pdf;Snapshot:/Users/ellingsv/Zotero/storage/BG7HDY4A/1603.html:text/html
          },
}
@misc{standev2018rstan,
  title = {{RStan}: the {R} interface to {Stan}},
  author = {{Stan Development Team}},
  note = {R package version 2.17.3},
  year = {2018},
  url = {http://mc-stan.org/},
}
